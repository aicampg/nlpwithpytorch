{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFNHCNaa7hoD",
    "tags": []
   },
   "source": [
    "# OHE, CountVectorizer\n",
    "\n",
    "This notebook is in several parts as follows:\n",
    "1) In Part 1, we cover the basics of tokenization and one-hot encoding\n",
    "2) In Part 2, we cover CountVectorizer\n",
    "3) In Part 3, we cover end-to-end disaster data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Tokenization & OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1603920515442,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 668,
    "execution_start": 1603584265309,
    "id": "fljRUYmG7hoF",
    "output_cleared": false,
    "source_hash": "a45df8bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1603920542944,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 2,
    "execution_start": 1603584592441,
    "id": "KGsAdCbR7hoL",
    "output_cleared": false,
    "source_hash": "60de3b07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_text = \"\"\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way â€“ in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\"\"\"\n",
    "short_text = \"\"\"In fairy-tales, witches always wear silly black hats and black coats, and they ride on broomsticks. But this is not a fairy-tale. This is about REAL WITCHES.\"\"\"\n",
    "text = short_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tJVllsO7hoO",
    "tags": []
   },
   "source": [
    "#### Tokenization\n",
    "\n",
    "ML algos tend not to work with categorical data. Rather than working with categorical data directly, **encoding** allows you to _more expressively_ represent the text (categorical) data. We do this by converting categories to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1603920551576,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 1,
    "execution_start": 1603584593689,
    "id": "wlDen1s27hoO",
    "output_cleared": false,
    "source_hash": "dc0df4a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    temp = text.split()\n",
    "    text_words = []\n",
    "\n",
    "    for word in temp:\n",
    "        # remove punctuation at beginning of word\n",
    "        while word[0] in string.punctuation:\n",
    "            word = word[1:]\n",
    "\n",
    "        # remove punctuation at end of word\n",
    "        while word[-1] in string.punctuation:\n",
    "            word = word[:-1]\n",
    "\n",
    "        # Append this word into our list of words\n",
    "        text_words.append(word.lower())\n",
    "\n",
    "    return text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1603917471984,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 3,
    "execution_start": 1603584595683,
    "id": "Bj7D65WP7hoR",
    "outputId": "02cd6f0c-d619-4dda-fc9a-469ea7675726",
    "output_cleared": false,
    "source_hash": "ea3be4d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_words = extract_words(text)\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMVWm1MK7hod",
    "tags": []
   },
   "source": [
    "#### One Hot Encoding\n",
    "\n",
    "1. is a more efficient way to represent vectors. \n",
    "2. the column feature vector defines a high dimensional space, where each dimension represents a word\n",
    "3. each element is zero in this vector, except the element corresponding to the dimension representing the word\n",
    "4. For _full-texts_ instead of words, the vector representation of the text is simply the vector sum of all the words it contains:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1603920565223,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 3,
    "execution_start": 1603584606565,
    "id": "ITh5DJ317hoU",
    "output_cleared": false,
    "source_hash": "52c2a925",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "word_list = []\n",
    "vocabulary_size = 0\n",
    "text_tokens = []\n",
    "\n",
    "for word in text_words:\n",
    "    # create an ID for words seen for the first time & add to dictionary\n",
    "    if word not in word_dict:\n",
    "        word_dict[word] = vocabulary_size\n",
    "        word_list.append(word)\n",
    "        vocabulary_size += 1\n",
    "\n",
    "    # add the token corresponding to the current word to the tokenized text.\n",
    "    text_tokens.append(word_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1603920566836,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 12,
    "execution_start": 1603584607171,
    "id": "GcWaKMbY7hoX",
    "outputId": "caffedae-bb5a-48ee-a470-675938c915eb",
    "output_cleared": false,
    "source_hash": "34ea0828",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Word list:\", word_list, \"\\n\\n Word dictionary\")\n",
    "pprint(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1603917476342,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 9,
    "execution_start": 1603584664464,
    "id": "7N--gIfW7hoa",
    "outputId": "2a0df5d4-2745-4c60-eec7-cdba66cb14ae",
    "output_cleared": false,
    "source_hash": "4f6f3b19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1603920610221,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_start": 1603594823770,
    "id": "6k3EncFh7hoe",
    "output_cleared": false,
    "source_hash": "443579d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"\"\"\n",
    "Mary had a little lamb, little lamb,\n",
    "little lamb, Mary had a little lamb\n",
    "whose fleece was white as snow. \n",
    "And everywhere that Mary went\n",
    "Mary went, Mary went, everywhere \n",
    "that Mary went\n",
    "the lamb was sure to go\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1603920617783,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 2,
    "execution_start": 1603595221022,
    "id": "Lp7LA6617hoh",
    "output_cleared": false,
    "source_hash": "ea886b5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = re.sub(r'[^\\w\\s]', '', text) \n",
    "word_list = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1603920620352,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 2,
    "execution_start": 1603595287689,
    "id": "Xqey9aJc7hoj",
    "output_cleared": false,
    "source_hash": "4a957f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for word in word_list:\n",
    "    if not word in word_dict.keys():\n",
    "        word_dict[word] = 1\n",
    "    else:\n",
    "        word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1603920621477,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 6,
    "execution_start": 1603595426186,
    "id": "ldlsFONL7hom",
    "output_cleared": false,
    "source_hash": "6be3920c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot(word, word_dict):\n",
    "    \"\"\"\n",
    "    Generate a one-hot encoded vector for \"word\"\n",
    "    \"\"\"\n",
    "\n",
    "    vector = np.zeros(len(word_dict))\n",
    "    vector[word_dict[word]] = 1\n",
    "    return vector\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1603920626800,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 1,
    "execution_start": 1603596226977,
    "id": "D4Bjlp4h7hop",
    "outputId": "eb00e216-3525-4354-9c09-e01394ed9b1c",
    "output_cleared": false,
    "source_hash": "6911aa75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fleece_hot = one_hot('fleece', word_dict)\n",
    "print(fleece_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1603920649053,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 1,
    "execution_start": 1603597039485,
    "id": "3Udez54W7hov",
    "outputId": "a1a47404-4302-45a6-8868-429fc6cc67ec",
    "output_cleared": false,
    "source_hash": "2b482a4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mary_hot = one_hot('mary', word_dict)\n",
    "print(mary_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1603917499384,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 12,
    "execution_start": 1603597168124,
    "id": "UYZvJ0Fr7ho0",
    "outputId": "fc2901ed-4629-4197-ef36-33ebc4192aa4",
    "output_cleared": false,
    "source_hash": "44ce6447",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mary_hot[6] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE with Scikit-learn\n",
    "\n",
    "1. Pre-process (transform) the data\n",
    "    a. Lower case\n",
    "    b. Split by space\n",
    "2. Integer encode using `fit_transform` on the dataset\n",
    "3. Alternatively, use binary encoding\n",
    "\n",
    "`LabelEncoder` can be used to normalize labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder # Try ?LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. convert corpus documents to lower case\n",
    "doc1 = long_text.lower()\n",
    "doc2 = short_text.lower()\n",
    "\n",
    "# convert to unary tokens\n",
    "doc1 = doc1.split()\n",
    "doc2 = doc2.split()\n",
    "dataset = array(doc1+doc2) # convert the list to a numpy array of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Integer encode the dataset\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Option 1: Fit, then transform -- 2 steps\n",
    "\n",
    "## Fit label encoder\n",
    "le = label_encoder.fit(dataset) # create an instance of the class based on the dataset\n",
    "le_classes = le.classes_\n",
    "\n",
    "## Transform labels to normalized encoding\n",
    "integer_encoded = le.transform(dataset)\n",
    "\n",
    "print(f'Corpus vocabulary\\n\\n{le.classes_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 -> single command: fit_transform\n",
    "\n",
    "## Fit label encoder and return encoded labels\n",
    "integer_encoded = label_encoder.fit_transform(dataset) # alphabetically assign integers to each word\n",
    "\n",
    "print(f'doc1 & doc2 encoded as integers: \\n\\n{integer_encoded}')\n",
    "print(f'\\n1D shape of integer_encoded: {integer_encoded.shape}') #1D shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Binary encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "## Reshape `integer_encoded` into a 2D array required for `OneHotEncoder` instance\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(f'First couple of rows of OHE docs: \\n\\n{onehot_encoded[:2,:]}') # convert to an array of size integer_encoded x max(integer_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert\n",
    "\n",
    "## Transform labels back to original encoding\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(f'Verify that the encoded values match the text: \\n{inverted}->{doc1[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "So where do we go from here? \n",
    "\n",
    "1. As you can see, we have managed to __encode__ the text corpus into a numerical vector format. \n",
    "2. The text was broken into unary tokens (you could also create n-ary tokens), which were then encoded into integers, and from which we created OHE vectors - all zeros, save for the index representing the token\n",
    "3. Next, we could feed this into our neural network - the size of the input being the length of each vector. \n",
    "4. Though, not in scope for our discussion as we will move to TF-IDF and other topics, you could advance your programming skills by following this tutorial on implementation: [Reference](https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e) using Airline Tweet Analysis\n",
    "5. See below for the filesfor you to get started\n",
    "    a. You can download the data directly from Kaggle: [Reference](https://www.kaggle.com/general/74235)\n",
    "    b. Alternatively, download from the course's repo (`data`) folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scikit-learn: CountVectorizer\n",
    "\n",
    "As opposed to generating Label classes and then integer encoding those classes to one hot encoding vectors, you can use sklearn's `CountVectorizer` feature extraction on text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Each document is an element of the corpus:\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# or use our documents\n",
    "corpus = [long_text, short_text]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "corpus_features = vectorizer.get_feature_names()\n",
    "print(f'Corpus features:\\n\\n{corpus_features}')\n",
    "\n",
    "term_freq = X.toarray()\n",
    "print(f'\\nTerm Frequency: \\n\\n{term_freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram implementation with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Example NLP problem: end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/twitter_disaster/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster Tweet sample: \n",
    "train_df[train_df[\"target\"]==1][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a disaster sample\n",
    "train_df[train_df[\"target\"]==0][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "\n",
    "1. Load dataset\n",
    "2. Divide the dataset into test & train components\n",
    "3. Using sklearn's `CountVectorizer`, count the number of words in each tweet and turn into numerical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the dataset as a pandas DataFrame\n",
    "def load_dataset(filename, text=\"text\", target=\"target\"):\n",
    "    data = pd.read_csv(filename) # header=None, if needed\n",
    "    X = data[text]\n",
    "    y = data[target]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset('data/twitter_disaster/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "X_train_enc = count_vectorizer.fit_transform(X_train)\n",
    "X_test_enc = count_vectorizer.transform(X_test) # note, using only transform, not fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train & Test integer encoding to vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_integer_encoded_vectors = count_vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**, we use `transform`, instead of `fit_transform` for the test data \n",
    "\n",
    "This ensures that only tokens from the _training data_ are mapped to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_integer_encoded_vectors = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_integer_encoded_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_integer_encoded_vectors[0].todense().shape)\n",
    "print(train_integer_encoded_vectors[0].todense())\n",
    "print(f'There are {train_integer_encoded_vectors[0].todense().shape[1]} unique words in the {len(train_df)} tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "- Using *ridge regression* model which allows our huge vectors to push the model weights toward zero without completely discounting different words\n",
    "- Ridge regression adds a \"ridge\" term that has the effect of \"smoothing\" the weights \n",
    "- Equivalent to training a linear model with weight decay that decreases variance, at the cost of increasing bias a small amount\n",
    "- For details, this explanation may help: https://www.youtube.com/watch?v=Q81RR3yKn30\n",
    "\n",
    "#### Linear model vs Ridge Regression\n",
    "- Linear model minimizes sum of squared residuals: $y \\leftarrow C*X + B$\n",
    "- Ridge regression adds a $\\lambda * slope^2$ penalty to the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(classifier_model, X_train_enc, y_train, cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the model looks good, then let's fit it to the training dataset\n",
    "classifier_model.fit(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(prediction, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_inefficient.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "66d78655-f453-4d60-8669-c1f19503d6ca",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
