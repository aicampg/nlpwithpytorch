{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "matched-jefferson",
   "metadata": {},
   "source": [
    "# SpaCy Pipelines\n",
    "\n",
    "![pipeline](images/pipeline.png)\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it.\n",
    "\n",
    "**Built-in Pipeline Components**\n",
    "\n",
    "| Name        | Description             | Creates                                                   |\n",
    "| ----------- | ----------------------- | --------------------------------------------------------- |\n",
    "| **tagger**  | Part-of-speech tagger   | `Token.tag`, `Token.pos`                                  |\n",
    "| **parser**  | Dependency parser       | `Token.dep`, `Token.head`, `Doc.sents`, `Doc.noun_chunks` |\n",
    "| **ner**     | Named entity recognizer | `Doc.ents`, `Token.ent_iob`, `Token.ent_type`             |\n",
    "| **textcat** | Text classifier         | `Doc.cats`                                                |\n",
    "\n",
    "spaCy ships with the following built-in pipeline components.\n",
    "\n",
    "* The part-of-speech tagger sets the token.tag and token.pos attributes.\n",
    "* The dependency parser adds the token.dep and token.head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "* The named entity recognizer adds the detected entities to the doc.ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "* Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc.cats property.\n",
    "* Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system.\n",
    "\n",
    "All models include a `meta.json` file:\n",
    "* Pipeline defined in model's meta.json in order\n",
    "* Built-in components need binary data to make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-composer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
